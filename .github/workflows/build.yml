name: Build Flash Attention 3 Wheel

on:
  push:
    branches: [ master ]
    tags: [ '*' ]
  pull_request:
    branches: [ master ]

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    permissions:
      contents: write
      packages: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v5
      with:
        submodules: recursive

    - name: Show disk and memory usage
      run: |
        echo "Disk usage (/):"; df -h /
        echo "Disk usage (workspace):"; df -h .
        echo "Memory usage:"; free -h

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker image
      run: |
        docker build --target builder -t flash-attention-3-builder .

    - name: Create container and extract wheel
      run: |
        CONTAINER_ID=$(docker create flash-attention-3-builder)
        docker cp $CONTAINER_ID:/workspace/flash-attention/hopper/dist/. ./dist/
        docker rm $CONTAINER_ID

    - name: Get build info
      run: |
        BASE_IMAGE=$(awk -F'=' '/^ARG BASE_IMAGE=/ {print $2}' Dockerfile)
        CUDA_VERSION=$(echo $BASE_IMAGE | sed 's/.*cuda:\([^ -]*\).*/\1/')
        PYTHON_VERSION=$(awk -F'=' '/^ARG PYTHON_VERSION=/ {print $2}' Dockerfile)
        TORCH_VERSION=$(awk -F'=' '/^ARG TORCH_VERSION=/ {print $2}' Dockerfile)
        echo "CUDA_VERSION=$CUDA_VERSION" >> $GITHUB_ENV
        echo "PYTHON_VERSION=$PYTHON_VERSION" >> $GITHUB_ENV
        echo "TORCH_VERSION=$TORCH_VERSION" >> $GITHUB_ENV
        echo "COMMIT_SHA=${{ github.sha }}" >> $GITHUB_ENV
        cd flash-attention/hopper
        echo "FLASH_ATTENTION_SHA=$(git rev-parse HEAD)" >> $GITHUB_ENV

    - name: Upload wheel as artifact
      uses: actions/upload-artifact@v5
      with:
        name: flash-attention-3-wheel
        path: dist/*.whl

    - name: Create latest release for main
      if: github.ref == 'refs/heads/master'
      uses: softprops/action-gh-release@v2
      with:
        tag_name: latest
        name: ${{ github.sha }}
        body: |
          Latest Flash Attention 3 wheel from main branch (commit ${{ github.sha }})

          **Build Info:**
          - CUDA Version: ${{ env.CUDA_VERSION }}
          - Python Version: ${{ env.PYTHON_VERSION }}
          - Torch Version: ${{ env.TORCH_VERSION }}
          - Docker Image SHA: ${{ env.COMMIT_SHA }}
          - Flash Attention 3 Git Hash: ${{ env.FLASH_ATTENTION_SHA }}
        files: dist/*.whl
        draft: false
        prerelease: true

    - name: Create release for tag
      if: startsWith(github.ref, 'refs/tags/')
      uses: softprops/action-gh-release@v2
      with:
        tag_name: ${{ github.ref_name }}
        name: Release ${{ github.ref_name }}
        body: |
          Flash Attention 3 wheel for ${{ github.ref_name }}

          **Build Info:**
          - CUDA Version: ${{ env.CUDA_VERSION }}
          - Python Version: ${{ env.PYTHON_VERSION }}
          - Torch Version: ${{ env.TORCH_VERSION }}
          - Docker Image SHA: ${{ env.COMMIT_SHA }}
          - Flash Attention 3 Git Hash: ${{ env.FLASH_ATTENTION_SHA }}
        files: dist/*.whl
        draft: false
        prerelease: false

  test:
    runs-on: ubuntu-latest
    needs: build
    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Install uv
      run: |
        curl -L https://github.com/astral-sh/uv/releases/latest/download/uv-x86_64-unknown-linux-gnu.tar.gz | tar -xz -C /usr/local/bin --strip-components=1 uv-x86_64-unknown-linux-gnu/uv

    - name: Get build info
      run: |
        BASE_IMAGE=$(awk -F'=' '/^ARG BASE_IMAGE=/ {print $2}' Dockerfile)
        CUDA_VERSION=$(echo $BASE_IMAGE | sed 's/.*cuda:\([^ -]*\).*/\1/')
        PYTHON_VERSION=$(awk -F'=' '/^ARG PYTHON_VERSION=/ {print $2}' Dockerfile)
        TORCH_VERSION=$(awk -F'=' '/^ARG TORCH_VERSION=/ {print $2}' Dockerfile)
        echo "CUDA_VERSION=$CUDA_VERSION" >> $GITHUB_ENV
        echo "PYTHON_VERSION=$PYTHON_VERSION" >> $GITHUB_ENV
        echo "TORCH_VERSION=$TORCH_VERSION" >> $GITHUB_ENV
        echo "COMMIT_SHA=${{ github.sha }}" >> $GITHUB_ENV
        cd flash-attention/hopper
        echo "FLASH_ATTENTION_SHA=$(git rev-parse HEAD)" >> $GITHUB_ENV

    - name: Download wheel from latest release
      run: gh release download latest --pattern "*.whl" --dir .
      env:
        GH_TOKEN: ${{ github.token }}

    - name: Set up test project
      run: |
        mkdir test
        mv *.whl test/
        cd test
        cat > pyproject.toml << EOF
        [project]
        name = "test"
        version = "0.1.0"
        dependencies = ["torch==${{ env.TORCH_VERSION }}"]
        requires-python = ">=${{ env.PYTHON_VERSION }}"
        EOF
        uv add *.whl

    - name: Test import
      run: |
        cd test
        uv run python -c "
        import flash_attn_3
        import flash_attn_interface
        import inspect
        print('Flash attention 3 import successful')
        print('Available interface functions:', [name for name in dir(flash_attn_interface) if not name.startswith('_')])
        print('flash_attn_varlen_func signature:', inspect.signature(flash_attn_interface.flash_attn_varlen_func))
        "

    - name: Check for GPU
      run: |
        if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
          echo "GPU_AVAILABLE=true" >> $GITHUB_ENV
        else
          echo "GPU_AVAILABLE=false" >> $GITHUB_ENV
        fi

    - name: Run GPU test
      if: env.GPU_AVAILABLE == 'true'
      run: |
        cd test
        uv run python -c "
        import torch
        print('Torch CUDA available:', torch.cuda.is_available())
        if torch.cuda.is_available():
          import flash_attn_3
          import flash_attn_interface
          print('Flash attention 3 GPU import successful')
          # Basic functional test using varlen function
          batch_size, seqlen, nheads, headdim = 2, 64, 8, 64
          total_q = batch_size * seqlen
          total_k = batch_size * seqlen
          
          q = torch.randn(total_q, nheads, headdim, device='cuda', dtype=torch.float16)
          k = torch.randn(total_k, nheads, headdim, device='cuda', dtype=torch.float16)
          v = torch.randn(total_k, nheads, headdim, device='cuda', dtype=torch.float16)
          
          # Create cumulative sequence lengths
          cu_seqlens_q = torch.tensor([0, seqlen, 2 * seqlen], device='cuda', dtype=torch.int32)
          cu_seqlens_k = torch.tensor([0, seqlen, 2 * seqlen], device='cuda', dtype=torch.int32)
          
          try:
            out = flash_attn_interface.flash_attn_varlen_func(
              q, k, v, cu_seqlens_q, cu_seqlens_k, seqlen, seqlen,
              dropout_p=0.0, causal=False
            )
            print('Flash attention 3 varlen function ran successfully on random data')
            print('Output shape:', out.shape)
          except Exception as e:
            print('Flash attention 3 varlen function test failed:', str(e))
            print('Available interface functions:', [name for name in dir(flash_attn_interface) if not name.startswith('_')])
        else:
          print('CUDA not available')
        "